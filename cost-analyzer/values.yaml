global:
  # zone: cluster.local (use only if your DNS server doesn't live in the same zone as kubecost)

  ## @section Prometheus
  ## @param global.prometheus.enabled If false, Prometheus will not be installed -- only actively supported on paid Kubecost plans
  ## @param global.prometheus.fqdn example address of a prometheus to connect to. Include protocol (http:// or https://) Ignored if enabled: true
  ## @extra global.prometheus.insecureSkipVerify If true, kubecost will not check the TLS cert of prometheus
  ## @param global.prometheus.queryServiceBasicAuthSecretName [string] kubectl create secret generic dbsecret -n kubecost --from-file=USERNAME --from-file=PASSWORD
  ## @param global.prometheus.queryServiceBearerTokenSecretName [string] kubectl create secret generic mcdbsecret -n kubecost --from-file=TOKEN
  prometheus:
    enabled: true
    fqdn: http://cost-analyzer-prometheus-server.default.svc
    # insecureSkipVerify: false
    # queryServiceBasicAuthSecretName: dbsecret
    # queryServiceBearerTokenSecretName: dbsecret

  ## @section Thanos
  # Durable storage option, product key required
  ## @param global.thanos.enabled Enable Thanos sub-chart to be installed -- only supported on paid Kubecost plans
  ## @param global.thanos.queryService [string] an address of the thanos query-frontend endpoint, if different from installed thanos
  ## @param global.thanos.queryServiceBasicAuthSecretNamekubectl [string] create secret generic mcdbsecret -n kubecost --from-file=USERNAME --from-file=PASSWORD <---enter basic auth credentials like that
  ## @param global.thanos.queryServiceBearerTokenSecretName [string] kubectl create secret generic mcdbsecret -n kubecost --from-file=TOKEN
  ## @param global.thanos.queryOffset [string] The offset to apply to all thanos queries in order to achieve syncronization on all cluster block stores
  thanos:
    enabled: false
    # queryService: http://kubecost-thanos-query-frontend-http.kubecost:{{ .Values.thanos.queryFrontend.http.port }}
    # queryServiceBasicAuthSecretName: mcdbsecret
    # queryServiceBearerTokenSecretName mcdbsecret
    # queryOffset: 3h

  ## @section Grafana
  ## @param global.grafana.enabled If false, Grafana will not be installed
  ## @param global.grafana.domainName example grafana domain Ignored if enabled: true
  ## @param global.grafana.scheme http or https, for the domain name above.
  ## @param global.grafana.proxy If true, the kubecost frontend will route to your grafana through its service endpoint
  grafana:
    enabled: true
    domainName: "cost-analyzer-grafana.default.svc"
    scheme: "http"
    proxy: true

  ## @section Kubecost Application Configuration
  ## @extra global.notifications Kubecost notification settings
  notifications:
    ## @param global.notifications.alertConfigs [object] Kubecost alerting configuration
    ## @param global.notifications.alertConfigs.frontendUrl [string] optional, used for linkbacks
    ## @param global.notifications.alertConfigs.globalSlackWebhookUrl [string] optional, used for Slack alerts
    ## @param global.notifications.alertConfigs.globalAlertEmails [array] optional, array of emails to use for Email alerts
    ## @param global.notifications.alertConfigs.alerts [array] Alerts generated by kubecost, about cluster data
    # Ref: http://docs.kubecost.com/alerts
    # alertConfigs:
      # frontendUrl: http://localhost:9090
      # globalSlackWebhookUrl: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX
      # globalAlertEmails:
      #   - recipient@example.com
      #   - additionalRecipient@example.com
      # alerts:
        # Daily namespace budget alert on namespace `kubecost`
        # - type: budget                # supported: budget, recurringUpdate
        #   threshold: 50               # optional, required for budget alerts
        #   window: daily               # or 1d
        #   aggregation: namespace
        #   filter: kubecost
        #   ownerContact:               # optional, overrides globalAlertEmails default
        #     - owner@example.com
        #     - owner2@example.com
        #   # optional, used for alert-specific Slack alerts
        #   slackWebhookUrl: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX

        # Daily cluster budget alert on cluster `cluster-one`
        # - type: budget
        #   threshold: 200.8        # optional, required for budget alerts
        #   window: daily           # or 1d
        #   aggregation: cluster
        #   filter: cluster-one     # does not accept csv

        # Recurring weekly update (weeklyUpdate alert)
        # - type: recurringUpdate
        #   window: weekly          # or 7d
        #   aggregation: namespace
        #   filter: '*'

        # Recurring weekly namespace update on kubecost namespace
        # - type: recurringUpdate
        #   window: weekly # or 7d
        #   aggregation: namespace
        #   filter: kubecost

        # Spend Change Alert
        # - type: spendChange         # change relative to moving avg
        #   relativeThreshold: 0.20   # Proportional change relative to baseline. Must be greater than -1 (can be negative)
        #   window: 1d                # accepts ‘d’, ‘h’
        #   baselineWindow: 30d       # previous window, offset by window
        #   aggregation: namespace
        #   filter: kubecost, default # accepts csv

        # Health Score Alert
        # - type: health              # Alerts when health score changes by a threshold
        #   window: 10m
        #   threshold: 5              # Send Alert if health scores changes by 5 or more

        # Kubecost Health Diagnostic
        # - type: diagnostic          # Alerts when kubecost is is unable to compute costs - ie: Prometheus unreachable
        #   window: 10m

    ## @param global.notifications.alertmanager.enabled If true, allow kubecost to write to your alertmanager
    ## @param global.notifications.alertmanager.fqdn example fqdn. Ignored if prometheus.enabled: true
    alertmanager: # Supply an alertmanager FQDN to receive notifications from the app.
      enabled: false
      fqdn: http://cost-analyzer-prometheus-server.default.svc

  ## @extra global.savedReports [object] Set saved allocation report(s) accessible from reports.html (Ref: http://docs.kubecost.com/saved-reports)
  ## @param global.savedReports.enabled If true, overwrites report parameters set through UI
  ## @param global.savedReports.reports [array] List of reports to create on pod startup
  savedReports:
    enabled: false
    reports:
      - title: "Example Saved Report 0"
        window: "today"
        aggregateBy: "namespace"
        idle: "separate"
        accumulate: false # daily resolution
        filters:
          - property: "cluster"
            value: "cluster-one,cluster*" # supports wildcard filtering and multiple comma separated values
          - property: "namespace"
            value: "kubecost"
      - title: "Example Saved Report 1"
        window: "month"
        aggregateBy: "controllerKind"
        idle: "share"
        accumulate: false
        filters:
          - property: "label"
            value: "app:cost*,environment:kube*"
          - property: "namespace"
            value: "kubecost"
      - title: "Example Saved Report 2"
        window: "2020-11-11T00:00:00Z,2020-12-09T23:59:59Z"
        aggregateBy: "service"
        idle: "hide"
        accumulate: true # entire window resolution
        filters: [] # if no filters, specify empty array

  ## @extra global.assetReports Set saved asset report(s) accessible from reports.html (Ref: http://docs.kubecost.com/saved-reports)
  ## @param global.assetReports.enabled If true, overwrites report parameters set through UI
  ## @param global.assetReports.reports [array] List of reports to create on pod startup
  assetReports:
    enabled: false
    reports:
    - title: "Example Asset Report 0"
      window: "today"
      aggregateBy: "type"
      accumulate: false # daily resolution
      filters:
        - property: "cluster"
          value: "cluster-one"

  ## @section Kubecost Deployment Configuration
  ## @param global.podAnnotations Additional annotations to add to to resources deployed by the chart
  ## @param global.additionalLabels Additional labels to add to to resources deployed by the chart
  podAnnotations: {}
    # iam.amazonaws.com/role: role-arn
  additionalLabels: {}

## @section Kubecost Application Configuration
## @param kubecostToken [string] generated at http://kubecost.com/install, used for alerts tracking and free trials
kubecostToken:

## @param pricingCsv.enabled Enable advanced pipeline for custom prices, enterprise key required
## @param pricingCsv.location.provider Provider for custom prices pipeline
## @param pricingCsv.location.region Region for custom prices pipeline
## @param pricingCsv.location.URI a valid file URI
## @param pricingCsv.location.csvAccessCredentials Access credentials for custom prices pipeline
pricingCsv:
  enabled: false
  location:
    provider: "AWS"
    region: "us-east-1"
    URI: s3://kc-csv-test/pricing_schema.csv
    csvAccessCredentials: pricing-schema-access-secret

## @param saml [object] SAML integration for user management and RBAC, enterprise key required
## @skip saml.enabled
## @skip saml.secretName
## @skip saml.idpMetadataURL
## @skip saml.appRootURL
## @skip saml.rbac.enabled
## @skip saml.rbac.groups
# Ref: https://github.com/kubecost/docs/blob/master/user-management.md
saml:
  enabled: false
  secretName: "kubecost-authzero"
  #metadataSecretName: "kubecost-authzero-metadata" # One of metadataSecretName or idpMetadataURL must be set. defaults to metadataURL if set
  idpMetadataURL: "https://dev-elu2z98r.auth0.com/samlp/metadata/c6nY4M37rBP0qSO1IYIqBPPyIPxLS8v2"
  appRootURL: "http://localhost:9090" # sample URL
  # audienceURI: "http://localhost:9090" # by convention, the same as the appRootURL, but any string uniquely identifying kubecost to your samp IDP. Optional if you follow the convention
  # nameIDFormat: "urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified" If your SAML provider requires a specific nameid format
  # isGLUUProvider: false # An additional URL parameter must be appended for GLUU providers
  rbac:
    enabled: false
    groups:
      - name: admin
        enabled: false # if admin is disabled, all SAML users will be able to make configuration changes to the kubecost frontend
        assertionName: "http://schemas.auth0.com/userType" # a SAML Assertion, one of whose elements has a value that matches on of the values in assertionValues
        assertionValues:
          - "admin"
          - "superusers"
      - name: readonly
        enabled: false # if readonly is disabled, all users authorized on SAML will default to readonly
        assertionName:  "http://schemas.auth0.com/userType"
        assertionvalues:
          - "readonly"

## @section Kubecost Deployment Configuration
## @skip systemProxy.enabled
## @skip systemProxy.httpProxyUrl
## @skip systemProxy.httpsProxyUrl
## @skip systemProxy.noProxy
# Adds an httpProxy as an environment variable. systemProxy.enabled must be `true`to have any effect.
# Ref: https://www.oreilly.com/library/view/security-with-go/9781788627917/5ea6a02b-3d96-44b1-ad3c-6ab60fcbbe4f.xhtml
systemProxy:
  enabled: false
  httpProxyUrl: ""
  httpsProxyUrl: ""
  noProxy: ""

# imagePullSecrets:
# - name: "image-pull-secret"

## @skip kubecostFrontend.image
## @skip kubecostFrontend.imagePullPolicy
## @skip kubecostFrontend.resources.requests.cpu
## @skip kubecostFrontend.resources.requests.memory
kubecostFrontend:
  image: "gcr.io/kubecost1/frontend"
  imagePullPolicy: Always
  resources:
    requests:
      cpu: "10m"
      memory: "55Mi"
    #limits:
    #  cpu: "100m"
    #  memory: "256Mi"

## @skip kubecost.disableServer
## @skip kubecost.image
## @skip kubecost.resources.requests.cpu
## @skip kubecost.resources.requests.memory
kubecost:
  # Disables the cost-analyzer-server container to spin up as part of kubecost
  # Setting this to true will mean all /api/ endpoints will be unavailable
  disableServer: false
  image: "gcr.io/kubecost1/server"
  resources:
    requests:
      cpu: "100m"
      memory: "55Mi"
    #limits:
    #  cpu: "100m"
    #  memory: "256Mi"

## @skip kubecostMetrics.exporter.enabled
## @skip kubecostMetrics.exporter.port
## @skip kubecostMetrics.exporter.resources
## @skip kubecostMetrics.exporter.tolerations
## @skip kubecostMetrics.exporter.affinity
## @skip kubecostMetrics.exporter.serviceMonitor.enabled
## @skip kubecostMetrics.exporter.serviceMonitor.additionalLabels
## @skip kubecostMetrics.exporter.priorityClassName
## @skip kubecostMetrics.exporter.additionalLabels
## @skip kubecostMetrics.exporter.nodeSelector
## @skip kubecostMetrics.exporter.annotations
# Kubecost Metrics deploys a separate pod which will emit kubernetes specific metrics required 
# by the cost-model. This pod is designed to remain active and decoupled from the cost-model itself. 
# However, disabling this service/pod deployment will flag the cost-model to emit the metrics instead. 
kubecostMetrics: 
  # emitPodAnnotations: false 
  # emitNamespaceAnnotations: false 
  # emitKsmV1Metrics: true
  
  # Optional
  # The metrics exporter is a separate deployment and service (for prometheus scrape auto-discovery)
  # which emits metrics cost-model relies on. Enabling this deployment also removes the KSM dependency
  # from the cost-model. If the deployment is not enabled, the metrics will continue to be emitted from
  # the cost-model.
  exporter:
    enabled: false
    port: 9005
    resources: {}
      # requests:
      #  cpu: "200m"
      #  memory: "55Mi"
    ## Node tolerations for server scheduling to nodes with taints
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    tolerations: []

    #  - key: "key"
    #    operator: "Equal|Exists"
    #    value: "value"
    #    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
    affinity: {}

    # Service Monitor for Kubecost Metrics 
    serviceMonitor:
      enabled: false
      additionalLabels: {}

    ## PriorityClassName
    ## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
    priorityClassName: []
    additionalLabels: {}
    nodeSelector: {}
    annotations: {}

## @skip kubecostModel.image
## @skip kubecostModel.imagePullPolicy
## @skip kubecostModel.outOfClusterPromMetricsEnabled
## @skip kubecostModel.warmCache
## @skip kubecostModel.warmSavingsCache
## @skip kubecostModel.etl
## @skip kubecostModel.etlDailyStoreDurationDays
## @skip kubecostModel.etlHourlyStoreDurationHours
## @skip kubecostModel.maxQueryConcurrency
## @skip kubecostModel.resources.requests.cpu
## @skip kubecostModel.resources.requests.memory
kubecostModel:
  image: "gcr.io/kubecost1/cost-model"
  imagePullPolicy: Always
  # Enables the emission of the kubecost_cloud_credit_total and
  # kubecost_cloud_expense_total metrics
  outOfClusterPromMetricsEnabled: false
  # Build local cost allocation cache
  warmCache: false
  # Build local savings cache
  warmSavingsCache: true
  # Run allocation ETL pipelines
  etl: true
  # The total number of days the ETL pipelines will build
  # Set to 0 to disable daily ETL (not recommended)
  etlDailyStoreDurationDays: 91
  # The total number of hours the ETL pipelines will build
  # Set to 0 to disable hourly ETL (not recommended)
  etlHourlyStoreDurationHours: 49
  # max number of concurrent Prometheus queries
  maxQueryConcurrency: 5
  resources:
    requests:
      cpu: "200m"
      memory: "55Mi"
    #limits:
    #  cpu: "800m"
    #  memory: "256Mi"

## @section Kubecost Deployment Configuration
# Basic Kubecost ingress, more examples available at https://github.com/kubecost/docs/blob/master/ingress-examples.md
## @param ingress.enabled If true, Ingress will be created
## @param ingress.annotations Ingress annotations
## @param ingress.paths Ingress paths
## @skip ingress.pathType
## @param ingress.hosts Ingress hostnames
## @param ingress.tls Ingress TLS configuration (YAML)
ingress:
  enabled: false
  # className: nginx
  annotations:
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  paths: ["/"] # There's no need to route specifically to the pods-- we have an nginx deployed that handles routing
  pathType: ImplementationSpecific
  hosts:
    - cost-analyzer.local
  tls: []
  #  - secretName: cost-analyzer-tls
  #    hosts:
  #      - cost-analyzer.local

## @skip nodeSelector
nodeSelector: {}

## @param tolerations node taints to tolerate
tolerations: []
#  - key: "key"
#    operator: "Equal|Exists"
#    value: "value"
#    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

## @param affinity pod affinity
affinity: {}

## @param priority.enabled If true, creates a PriorityClass to be used by the cost-analyzer pod
## @skip priority.name
priority:
  enabled: false
  name: "" # Provide name of existing priority class only. If left blank, upstream chart will create one from default template.
  # value: 1000000

# If true, enable creation of NetworkPolicy resources.
## @param networkPolicy.enabled If true, create a NetworkPolicy to deny egress
## @param networkPolicy.costAnalyzer.enabled If true, create a newtork policy for cost-analzyer
## @param networkPolicy.costAnalyzer.annotations Annotations to be added to the network policy
## @param networkPolicy.costAnalyzer.additionalLabels Additional labels to be added to the network policy
## @param networkPolicy.costAnalyzer.ingressRules [array] A list of network policy ingress rules
## @param networkPolicy.costAnalyzer.egressRules [array] A list of network policy egress rules
## @skip networkPolicy.denyEgress
## @skip networkPolicy.sameNamespace
networkPolicy:
  enabled: false
  denyEgress: true # create a network policy that denies egress from kubecost
  sameNamespace: true # Set to true if cost analyser and prometheus are on the same namespace
#  namespace: kubecost # Namespace where prometheus is installed

  # Cost-analyzer specific vars using the new template
  costAnalyzer:
    enabled: false # If true, create a newtork policy for cost-analzyer
    annotations: {} # annotations to be added to the network policy
    additionalLabels: {} # additional labels to be added to the network policy
    # Examples rules:
    # ingressRules:
    #   - selectors: # allow ingress from self on all ports
    #     - podSelector:
    #         matchLabels:
    #           app.kubernetes.io/name: cost-analyzer
    #   - selectors: # allow egress access to prometheus
    #     - namespaceSelector:
    #         matchLabels:
    #           name: prometheus
    #       podSelector:
    #         matchLabels:
    #           app: prometheus
    #     ports:
    #       - protocol: TCP
    #         port: 9090
    # egressRules:
    #   - selectors: # restrict egress to inside cluster
    #     - namespaceSelector: {}

## @skip podSecurityPolicy.enabled
podSecurityPolicy:
  enabled: true

## @param extraVolumes A list of volumes to be added to the pod
##
extraVolumes: []
## @param extraVolumeMounts A list of volume mounts to be added to the pod
##
extraVolumeMounts: []

# Define persistence volume for cost-analyzer, more information at https://github.com/kubecost/docs/blob/master/storage.md
## @param persistentVolume.enabled If true, Kubecost will create a Persistent Volume Claim for product config data.
## @param persistentVolume.size Define PVC size for cost-analyzer
## @param persistentVolume.dbSize Define PVC size for cost-analyzer's flat file database
persistentVolume:
  size: 32Gi
  dbSize: 32.0Gi
  enabled: true # Note that setting this to false means configurations will be wiped out on pod restart.
  # storageClass: "-" #
  # existingClaim: kubecost-cost-analyzer # a claim in the same namespace as kubecost

## @skip service.type
## @skip service.port
## @skip service.targetPort
## @skip service.labels
## @skip service.annotations
service:
  type: ClusterIP
  port: 9090
  targetPort: 9090
  # nodePort:
  labels: {}
  annotations: {}

## @skip remoteWrite.postgres.enabled
## @skip remoteWrite.postgres.initImage
## @skip remoteWrite.postgres.initImagePullPolicy
## @skip remoteWrite.postgres.installLocal
## @skip remoteWrite.postgres.remotePostgresAddress
## @skip remoteWrite.postgres.persistentVolume.size
## @skip remoteWrite.postgres.auth.password
# Enabling long-term durable storage with Postgres requires an enterprise license
remoteWrite:
  postgres:
    enabled: false
    initImage: "gcr.io/kubecost1/sql-init"
    initImagePullPolicy: Always
    installLocal: true
    remotePostgresAddress: "" # ignored if installing locally
    persistentVolume:
      size: 200Gi
    auth:
      password: admin # change me

## @section Prometheus
## @skip prometheus.extraScrapeConfigs
## @extra prometheus.kube-state-metrics.disabled If false, deploy [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) for Kubernetes metrics
## @param prometheus.kube-state-metrics.resources [object] Set kube-state-metrics resource requests and limits.
## @param prometheus.server.resources Prometheus server resource requests and limits.
## @param prometheus.server.retention [object] Determines when to remove old data.
## @skip prometheus.server.global.scrape_interval
## @skip prometheus.server.global.scrape_timeout
## @skip prometheus.server.global.evaluation_interval
## @skip prometheus.server.global.external_labels.cluster_id
## @param prometheus.server.persistentVolume.enabled If true, Prometheus server will create a Persistent Volume Claim.
## @param prometheus.server.persistentVolume.size Prometheus server data Persistent Volume size. Default set to retain ~6000 samples per second for 15 days.
## @skip prometheus.server.extraArgs.query.max-concurrency
## @skip prometheus.server.extraArgs.query.max-samples
## @skip prometheus.server.tolerations
## @skip prometheus.alertmanager.enabled
## @param prometheus.alertmanager.persistentVolume.enabled If true, Alertmanager will create a Persistent Volume Claim.
## @param prometheus.nodeExporter.enabled If false, do not create NodeExporter daemonset.
## @param prometheus.nodeExporter.resources [object] Node exporter resource requests and limits.
## @skip prometheus.pushgateway.enabled
## @param prometheus.pushgateway.persistentVolume.enabled If true, Prometheus Pushgateway will create a Persistent Volume Claim.
## @skip prometheus.serverFiles.rules.groups
prometheus:
  extraScrapeConfigs: |
    - job_name: kubecost
      honor_labels: true
      scrape_interval: 1m
      scrape_timeout: 10s
      metrics_path: /metrics
      scheme: http
      dns_sd_configs:
      - names:
        - {{ template "cost-analyzer.serviceName" . }}
        type: 'A'
        port: 9003
    - job_name: kubecost-networking
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
      # Scrape only the the targets matching the following metadata
        - source_labels: [__meta_kubernetes_pod_label_app]
          action: keep
          regex:  {{ template "cost-analyzer.networkCostsName" . }}
  server:
    # If clusterIDConfigmap is defined, instead use user-generated configmap with key CLUSTER_ID
    # to use as unique cluster ID in kubecost cost-analyzer deployment.
    # This overrides the cluster_id set in prometheus.server.global.external_labels.
    # NOTE: This does not affect the external_labels set in prometheus config.
    # clusterIDConfigmap: cluster-id-configmap

    resources: {}
    # limits:
    #   cpu: 500m
    #   memory: 512Mi
    # requests:
    #   cpu: 500m
    #   memory: 512Mi
    global:
      scrape_interval: 1m
      scrape_timeout: 10s
      evaluation_interval: 1m
      external_labels:
        cluster_id: cluster-one # Each cluster should have a unique ID
    persistentVolume:
      size: 32Gi
      enabled: true
    extraArgs:
      query.max-concurrency: 1
      query.max-samples: 100000000
    tolerations: []
    #  - key: "key"
    #    operator: "Equal|Exists"
    #    value: "value"
    #    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
  alertmanager:
    enabled: false
    persistentVolume:
      enabled: true
  nodeExporter:
    enabled: true
  pushgateway:
    enabled: false
    persistentVolume:
      enabled: true
  serverFiles:
  #  prometheus.yml: # Sample block -- enable if using an in cluster durable store.
  #      remote_write:
  #        - url: "http://pgprometheus-adapter:9201/write"
  #          write_relabel_configs:
  #            - source_labels: [__name__]
  #              regex: 'container_.*_allocation|container_.*_allocation_bytes|.*_hourly_cost|kube_pod_container_resource_requests{resource="memory", unit="byte"}|container_memory_working_set_bytes|kube_pod_container_resource_requests{resource="cpu", unit="core"}|kube_pod_container_resource_requests|pod_pvc_allocation|kube_namespace_labels|kube_pod_labels'
  #              action: keep
  #          queue_config:
  #            max_samples_per_send: 1000
        #remote_read:
        #  - url: "http://pgprometheus-adapter:9201/read"
    rules:
      groups:
        - name: CPU
          rules:
            - expr: sum(rate(container_cpu_usage_seconds_total{container_name!=""}[5m]))
              record: cluster:cpu_usage:rate5m
            - expr: rate(container_cpu_usage_seconds_total{container_name!=""}[5m])
              record: cluster:cpu_usage_nosum:rate5m
            - expr: avg(irate(container_cpu_usage_seconds_total{container_name!="POD", container_name!=""}[5m])) by (container_name,pod_name,namespace)
              record: kubecost_container_cpu_usage_irate
            - expr: sum(container_memory_working_set_bytes{container_name!="POD",container_name!=""}) by (container_name,pod_name,namespace)
              record: kubecost_container_memory_working_set_bytes
            - expr: sum(container_memory_working_set_bytes{container_name!="POD",container_name!=""})
              record: kubecost_cluster_memory_working_set_bytes
        - name: Savings
          rules:
            - expr: sum(avg(kube_pod_owner{owner_kind!="DaemonSet"}) by (pod) * sum(container_cpu_allocation) by (pod))
              record: kubecost_savings_cpu_allocation
              labels:
                daemonset: "false"
            - expr: sum(avg(kube_pod_owner{owner_kind="DaemonSet"}) by (pod) * sum(container_cpu_allocation) by (pod)) / sum(kube_node_info)
              record: kubecost_savings_cpu_allocation
              labels:
                daemonset: "true"
            - expr: sum(avg(kube_pod_owner{owner_kind!="DaemonSet"}) by (pod) * sum(container_memory_allocation_bytes) by (pod))
              record: kubecost_savings_memory_allocation_bytes
              labels:
                daemonset: "false"
            - expr: sum(avg(kube_pod_owner{owner_kind="DaemonSet"}) by (pod) * sum(container_memory_allocation_bytes) by (pod)) / sum(kube_node_info)
              record: kubecost_savings_memory_allocation_bytes
              labels:
                daemonset: "true"

## @section Kubecost Deployment Configuration
## Module for measuring network costs
## Ref: https://github.com/kubecost/docs/blob/master/network-allocation.md
## @param networkCosts.enabled If true, collect network allocation metrics [More info](http://docs.kubecost.com/network-allocation)
## @param networkCosts.podMonitor.enabled If true, a [PodMonitor](https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#podmonitor) for the network-cost daemonset is created
## @skip networkCosts.image
## @skip networkCosts.imagePullPolicy
## @skip networkCosts.prometheusScrape
## @skip networkCosts.trafficLogging
## @skip networkCosts.port
## @skip networkCosts.resources
## @skip networkCosts.config.destinations.in-zone
## @skip networkCosts.config.destinations.in-region
## @skip networkCosts.config.destinations.cross-region
## @skip networkCosts.config.destinations.direct-classification
## @skip networkCosts.config.services.google-cloud-services
## @skip networkCosts.config.services.amazon-web-services
## @skip networkCosts.config.services.azure-cloud-services
## @skip networkCosts.podMonitor.additionalLabels
## @skip networkCosts.podSecurityPolicy.enabled
## @skip networkCosts.tolerations
## @skip networkCosts.affinity
## @skip networkCosts.priorityClassName
## @skip networkCosts.additionalLabels
## @skip networkCosts.nodeSelector
## @skip networkCosts.annotations
## @skip networkCosts.updateStrategy.type
networkCosts:
  enabled: false
  podSecurityPolicy:
    enabled: false
  image: gcr.io/kubecost1/kubecost-network-costs:v16.0
  imagePullPolicy: Always
  updateStrategy:
    type: RollingUpdate
  # For existing Prometheus Installs, create a Service which generates Endpoints for each of the network-costs pods.
  # This Service is annotated with prometheus.io/scrape: "true" to automatically get picked up by the prometheus config.
  # NOTE: Setting this option to true and leaving the above extraScrapeConfig "job_name: kubecost-networking" configured will cause the
  # NOTE: pods to be scraped twice.
  prometheusScrape: false
  # Traffic Logging will enable logging the top 5 destinations for each source
  # every 30 minutes.
  trafficLogging: true
  # Port will set both the containerPort and hostPort to this value.
  # These must be identical due to network-costs being run on hostNetwork
  port: 3001
  resources: {}
    #requests:
    #  cpu: "50m"
    #  memory: "20Mi"
  config:
    # Configuration for traffic destinations, including specific classification
    # for IPs and CIDR blocks. This configuration will act as an override to the
    # automatic classification provided by network-costs.
    destinations:
      # In Zone contains a list of address/range that will be
      # classified as in zone.
      in-zone:
        # Loopback
        - "127.0.0.1"
        # IPv4 Link Local Address Space
        - "169.254.0.0/16"
        # Private Address Ranges in RFC-1918
        - "10.0.0.0/8" # Remove this entry if using Multi-AZ Kubernetes
        - "172.16.0.0/12"
        - "192.168.0.0/16"

      # In Region contains a list of address/range that will be
      # classified as in region. This is synonymous with cross
      # zone traffic, where the regions between source and destinations
      # are the same, but the zone is different.
      in-region: []

      # Cross Region contains a list of address/range that will be
      # classified as non-internet egress from one region to another.
      cross-region: []

      # Direct Classification specifically maps an ip address or range
      # to a region (required) and/or zone (optional). This classification
      # takes priority over in-zone, in-region, and cross-region configurations.
      direct-classification: []
      # - region: "us-east1"
      #   zone: "us-east1-c"
      #   ips:
      #     - "10.0.0.0/24"
    services:
      # google-cloud-services: when set to true, enables labeling traffic metrics with google cloud 
      # service endpoints 
      google-cloud-services: false
      # amazon-web-services: when set to true, enables labeling traffic metrics with amazon web service 
      # endpoints.
      amazon-web-services: false
      # azure-cloud-services: when set to true, enables labeling traffic metrics with azure cloud service 
      # endpoints 
      azure-cloud-services: false   
      # user defined services provide a way to define custom service endpoints which will label traffic metrics 
      # falling within the defined address range.
      #services: 
      #  - service: "test-service-1"
      #    ips:
      #      - "19.1.1.2"
      #  - service: "test-service-2"
      #    ips:
      #      - "15.128.15.2"
      #      - "20.0.0.0/8"

  ## Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
  #  - key: "key"
  #    operator: "Equal|Exists"
  #    value: "value"
  #    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  affinity: {}

  ## PriorityClassName
  ## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: []
  ## PodMonitor
  ## Allows scraping of network metrics from a dedicated prometheus operator setup
  podMonitor:
    enabled: false
    additionalLabels: {}
  additionalLabels: {}
  nodeSelector: {}
  annotations: {}

## @param kubecostDeployment.replicas Used for HA mode in Business & Enterprise tier
kubecostDeployment:
  replicas: 1

## @param clusterController.enabled If true, enable the Kubecost Cluster Controller for Right Sizing and Cluster Turndown
## @skip clusterController.image
## @skip clusterController.imagePullPolicy
clusterController:
  enabled: false
  image: gcr.io/kubecost1/cluster-controller:v0.0.2
  imagePullPolicy: Always

## @skip reporting.logCollection
## @skip reporting.productAnalytics
## @skip reporting.errorReporting
## @skip reporting.valuesReporting
reporting:
  # Kubecost bug report feature: Logs access/collection limited to .Release.Namespace
  # Ref: http://docs.kubecost.com/bug-report
  logCollection: true
  # Basic frontend analytics
  productAnalytics: true
  # Report Javascript errors
  errorReporting: true
  valuesReporting: true

## @param serviceMonitor.enabled Set this to true to create ServiceMonitor for Prometheus operator
## @param serviceMonitor.additionalLabels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
serviceMonitor:
  enabled: false
  additionalLabels: {}

## @param prometheusRule.enabled Set this to `true` to create PrometheusRule for Prometheus operator
## @param prometheusRule.additionalLabels Additional labels that can be used so PrometheusRule will be discovered by Prometheus
prometheusRule:
  enabled: false
  additionalLabels: {}

## @skip supportNFS
## @param initChownDataImage Ensures all Kubecost filepath permissions on PV or local storage are set up correctly. Supports a fully qualified Docker image, e.g. registry.hub.docker.com/library/busybox:latest
## @skip initChownData.resources
supportNFS: false
initChownDataImage: "busybox"
initChownData:
  resources: {}
    #requests:
    #  cpu: "50m"
    #  memory: "20Mi"


## @section Grafana
## @param grafana.resources [object] Grafana resource requests and limits.
## @param grafana.sidecar.datasources.defaultDatasourceEnabled Set this to `false` to disable creation of Prometheus datasource in Grafana
## @skip grafana.sidecar.dashboards.enabled
## @skip grafana.sidecar.dashboards.label
## @skip grafana.sidecar.dashboards.annotations
## @skip grafana.sidecar.dashboards.error_throttle_sleep
## @skip grafana.sidecar.datasources.enabled
## @skip grafana.sidecar.datasources.dataSourceName
## @skip grafana.sidecar.datasources.label
## @skip grafana.sidecar.datasources.error_throttle_sleep
## @skip grafana.grafana.ini.server.root_url
grafana:
  # namespace_datasources: kubecost # override the default namespace here
  # namespace_dashboards: kubecost # override the default namespace here
  sidecar:
    dashboards:
      enabled: true
      # label that the configmaps with dashboards are marked with
      label: grafana_dashboard
      # set sidecar ERROR_THROTTLE_SLEEP env var from default 5s to 0s -> fixes https://github.com/kubecost/cost-analyzer-helm-chart/issues/877
      annotations: {}
      error_throttle_sleep: 0
    datasources:
      # dataSourceFilename: foo.yml # If you need to change the name of the datasource file
      enabled: true
      defaultDatasourceEnabled: false
      dataSourceName: prometheus-kubecost
      # label that the configmaps with datasources are marked with
      label: kubecost_grafana_datasource
      # set sidecar ERROR_THROTTLE_SLEEP env var from default 5s to 0s -> fixes https://github.com/kubecost/cost-analyzer-helm-chart/issues/877
      error_throttle_sleep: 0
#  For grafana to be accessible, add the path to root_url. For example, if you run kubecost at www.foo.com:9090/kubecost
#  set root_url to "%(protocol)s://%(domain)s:%(http_port)s/kubecost/grafana". No change is necessary here if kubecost runs at a root URL
  grafana.ini:
    server:
      root_url: "%(protocol)s://%(domain)s:%(http_port)s/grafana"

## @section Kubecost Deployment Configuration
## @param serviceAccount.create Set this to `false` if you want to create the service account `kubecost-cost-analyzer` on your own
## @skip serviceAccount.annotations
serviceAccount:
  create: true # Set this to false if you're bringing your own service account.
  annotations: {}
  # name: kc-test

## @section Kubecost Application Configuration
## @skip awsstore.useAwsStore
## @skip awsstore.createServiceAccount
awsstore:
  useAwsStore: false
  createServiceAccount: false

#kubecostMetrics:
#  emitKsmV1Metrics: true # emit all KSM metrics in KSM v1.
#  emitKsmV1MetricsOnly: false # emit only the KSM metrics missing from KSM v2. Advanced users only.


# readonly: false # disable updates to kubecost from the frontend UI and via POST request

# These configs can also be set from the Settings page in the Kubecost product UI
# Values in this block override config changes in the Settings UI on pod restart
#
#kubecostProductConfigs:
# An optional list of cluster definitions that can be added for frontend access. The local
# cluster is *always* included by default, so this list is for non-local clusters.
# Ref: https://github.com/kubecost/docs/blob/master/multi-cluster.md
#  clusters:
#   - name: "Cluster A"
#     address: http://cluster-a.kubecost.com:9090
#     # Optional authentication credentials - only basic auth is currently supported.
#     auth:
#       type: basic
#       # Secret name should be a secret formatted based on: https://github.com/kubecost/docs/blob/master/ingress-examples.md
#       secretName: cluster-a-auth
#       # Or pass auth directly as base64 encoded user:pass
#       data: YWRtaW46YWRtaW4=
#       # Or user and pass directly
#       user: admin
#       pass: admin
#   - name: "Cluster B"
#     address: http://cluster-b.kubecost.com:9090
#  defaultModelPricing: # default monthly resource prices, used predominately for on-prem clusters
#    CPU: 28.0
#    spotCPU: 4.86
#    RAM: 3.09
#    spotRAM: 0.65
#    GPU: 693.50
#    spotGPU: 225.0
#    storage: 0.04
#    zoneNetworkEgress: 0.01
#    regionNetworkEgress: 0.01
#    internetNetworkEgress: 0.12
#    enabled: true
#  # The cluster profile represents a predefined set of parameters to use when calculating savings.
#  # Possible values are: [ development, production, high-availability ]
#  clusterProfile: production
#  customPricesEnabled: false # This makes the default view custom prices-- generally used for on-premises clusters
#  spotLabel: lifecycle
#  spotLabelValue: Ec2Spot
#  gpuLabel: gpu
#  gpuLabelValue: true
#  awsServiceKeyName: ACCESSKEYID
#  awsServiceKeyPassword:  fakepassword # Only use if your values.yaml are stored encrypted. Otherwise provide an existing secret via serviceKeySecretName
#  awsSpotDataRegion: us-east-1
#  awsSpotDataBucket: spot-data-feed-s3-bucket
#  awsSpotDataPrefix: dev
#  athenaProjectID: "530337586277" # The AWS AccountID where the Athena CUR is. Generally your masterpayer account
#  athenaBucketName: "s3://aws-athena-query-results-530337586277-us-east-1"
#  athenaRegion: us-east-1
#  athenaDatabase: athenacurcfn_athena_test1
#  athenaTable: "athena_test1"
#  masterPayerARN: ""
#  projectID: "123456789"  # Also known as AccountID on AWS -- the current account/project that this instance of Kubecost is deployed on.
#  gcpSecretName: gcp-secret # Name of a secret representing the gcp service key
#  bigQueryBillingDataDataset: billing_data.gcp_billing_export_v1_01AC9F_74CF1D_5565A2
#  labelMappingConfigs:  # names of k8s labels used to designate different allocation concepts
#    enabled: true
#    owner_label: "owner"
#    team_label: "team"
#    department_label: "dept"
#    product_label: "product"
#    environment_label: "env"
#    namespace_external_label: "kubernetes_namespace" # external labels are used to map external cloud costs to kubernetes concepts
#    cluster_external_label: "kubernetes_cluster"
#    controller_external_label: "kubernetes_controller"
#    product_external_label: "kubernetes_label_app"
#    service_external_label: "kubernetes_service"
#    deployment_external_label: "kubernetes_deployment"
#    owner_external_label: "kubernetes_label_owner"
#    team_external_label: "kubernetes_label_team"
#    environment_external_label: "kubernetes_label_env"
#    department_external_label: "kubernetes_label_department"
#    statefulset_external_label: "kubernetes_statefulset"
#    daemonset_external_label: "kubernetes_daemonset"
#    pod_external_label: "kubernetes_pod"
#  grafanaURL: ""
#  clusterName: "" # used for display in Kubecost UI
#  currencyCode: "USD" # offical support for USD, CAD, EUR, and CHF
#  azureBillingRegion: US # Represents 2-letter region code, e.g. West Europe = NL, Canada = CA. ref: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes
#  azureSubscriptionID: 0bd50fdf-c923-4e1e-850c-196dd3dcc5d3
#  azureClientID: f2ef6f7d-71fb-47c8-b766-8d63a19db017
#  azureTenantID: 72faf3ff-7a3f-4597-b0d9-7b0b201bb23a
#  azureClientPassword: fake key # Only use if your values.yaml are stored encrypted. Otherwise provide an existing secret via serviceKeySecretName
#  azureStorageSecretName: "azure-storage-config" # Name of Kubernetes Secret where Azure Storage Configuration is stored
#  discount: "" # percentage discount applied to compute
#  negotiatedDiscount: "" # custom negotiated cloud provider discount
#  defaultIdle: false
#  serviceKeySecretName: "" # Use an existing AWS or Azure secret with format as in aws-service-key-secret.yaml or azure-service-key-secret.yaml. Leave blank if using createServiceKeySecret
#  createServiceKeySecret: true # Creates a secret representing your cloud service key based on data in values.yaml. If you are storing unencrypted values, add a secret manually
#  sharedNamespaces: "" # namespaces with shared workloads, example value: "kube-system\,ingress-nginx\,kubecost\,monitoring"
#  sharedOverhead: "" # value representing a fixed external cost per month to be distributed among aggregations.
#  shareTenancyCosts: true # enable or disable sharing costs such as cluster management fees (defaults to "true" on Settings page)
#  productKey: # apply business or enterprise product license
#    key: ""
#    enabled: false
#    secretname: productkeysecret # create a secret out of a file named productkey.json of format { "key": "kc-b1325234" }
#  cloudIntegrationSecret: "cloud-integration"
